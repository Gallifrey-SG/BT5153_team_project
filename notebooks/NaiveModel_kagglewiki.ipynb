{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2289,"status":"ok","timestamp":1713253355968,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"MHjC_3NdRm39","outputId":"d61305ae-66e4-40c2-aeb0-45108a9eb55d"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713253355969,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"ZDJ_iLd7Rw8b"},"outputs":[],"source":["# import pandas as pd\n","# from distributed import Client, LocalCluster\n","# cluster = LocalCluster(memory_limit='8GB')\n","# client = Client(cluster)\n","import modin.pandas as pd\n","import modin.config as modin_cfg\n","modin_cfg.Engine.put(\"ray\")  # Modin will use Ray\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"elapsed":6,"status":"error","timestamp":1713253355969,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"rNnIzKLIRxxV","outputId":"3e004d85-922e-4533-8ef5-db431fe3ad7f"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-05 15:20:33,235\tINFO worker.py:1752 -- Started a local Ray instance.\n"]}],"source":["# kaggle_data = pd.read_csv('/content/drive/MyDrive/NUS_MSBA/BT5153_Final Group Project_Shared Folder/Data/kaggle_data.csv')\n","# wiki_data = pd.read_csv('/content/drive/MyDrive/NUS_MSBA/BT5153_Final Group Project_Shared Folder/Data/wiki_data.csv')\n","# essay = pd.read_csv('/content/drive/MyDrive/NUS_MSBA/BT5153_Final Group Project_Shared Folder/Data/competition_essay.csv')\n","# test_data = essay.drop(columns='Remark')\n","\n","train_data_processed = pd.read_csv(\"train_preprocessed_v2.csv\")\n","test_data_processed = pd.read_csv(\"new_essay_val_preprocessed.csv\")\n","test_data_dep_processed = pd.read_csv(\"test_data_preprocessed.csv\")"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1713253355969,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"JnsiyS8Pz7J2"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>school homework clubs become increasingly popu...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>widely accepted knowledge great source power s...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>first impressions great power shape interactio...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>name address city state zip code email address...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>limiting car usage numerous advantages benefit...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  school homework clubs become increasingly popu...    1.0\n","1  widely accepted knowledge great source power s...    1.0\n","2  first impressions great power shape interactio...    1.0\n","3  name address city state zip code email address...    1.0\n","4  limiting car usage numerous advantages benefit...    1.0"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# for each data, keep only text and label columns\n","train_data_processed = train_data_processed[['text', 'label']]\n","test_data_processed = test_data_processed[['text', 'label']]\n","test_data_dep_processed = test_data_dep_processed[['text', 'label']]\n","\n","train_data_processed.head()"]},{"cell_type":"markdown","metadata":{"id":"lYsKp7HC2xJ0"},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1713253355969,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"oc1iRFMKx_KW"},"outputs":[],"source":["# Data Preprocessing\n","# !pip install --upgrade pip\n","# !pip install transformers\n","# !pip install ftfy\n","# !pip install ax-platform"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1713253355969,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"N6galhMk3c_A"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/jiajiazhang/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/jiajiazhang/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","import string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from gensim import utils\n","import gensim.models\n","from ftfy import fix_text\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1713253355970,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"XOgbVzXm0qiH"},"outputs":[],"source":["def data_preprocessing(df):\n","    # Remove rows with any missing values\n","    df = df.dropna()\n","    # Drop duplicates where both 'text' and 'label' are the same\n","    df = df.drop_duplicates(subset=['text', 'label'])\n","    # Drop all entries where 'text' appears more than once (across different labels)\n","    df = df.drop_duplicates(subset='text', keep=False)\n","    return df"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1713253355970,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"7qWCpNcp1tvt"},"outputs":[],"source":["train_data = data_preprocessing(train_data_processed)\n","test_data = data_preprocessing(test_data_processed)\n","test_dep_data = data_preprocessing(test_data_dep_processed)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1713253355970,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"D9C_DhxNySr_"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>35</th>\n","      <td>wake mohamed bouazizis desperate act selfimmol...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>utilitarianism emphasis maximizing overall uti...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>rich countries long grappled question meaningf...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>literature change anything question whether ti...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>nurturing stability prosperity strengthening u...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                 text  label\n","35  wake mohamed bouazizis desperate act selfimmol...    1.0\n","36  utilitarianism emphasis maximizing overall uti...    1.0\n","37  rich countries long grappled question meaningf...    1.0\n","38  literature change anything question whether ti...    1.0\n","39  nurturing stability prosperity strengthening u...    1.0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# combine the text into corpus\n","df_list = [train_data, test_data]\n","text_corpus = pd.concat(df_list)\n","text_corpus.tail()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3061,"status":"aborted","timestamp":1713253355970,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"cInjYGlR2rww"},"outputs":[],"source":["# define meta feature function\n","class TextFeatureExtractor:\n","    def __init__(self):\n","        self.stopwords = set(stopwords.words('english'))\n","\n","    def transform(self, df):\n","        # Store original columns to keep after transformation\n","        original_columns = df.columns.tolist()\n","        # Compute various text-related features\n","        df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n","        df['unique_word_count'] = df['text'].apply(lambda x: len(set(str(x).split())))\n","        df['%unique_word_total']= df['unique_word_count']/df['word_count']\n","        df['stop_word_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in self.stopwords]))\n","        df['%stop_word_total']=df['stop_word_count']/df['word_count']\n","        df['mean_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","        df['char_count'] = df['text'].apply(lambda x: len(str(x)))\n","        df['mean_char_count_per_word']=df['char_count']/df['word_count']\n","        columns_to_keep = original_columns + ['%unique_word_total', '%stop_word_total','mean_word_length', 'mean_char_count_per_word']\n","        df = df[columns_to_keep]\n","        return df"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3061,"status":"aborted","timestamp":1713253355970,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"c8O829Zj3Npo"},"outputs":[],"source":["meta_feature_list = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length', 'char_count']\n","text_feature_extractor = TextFeatureExtractor()\n","\n","train_data = text_feature_extractor.transform(train_data)\n","test_data = text_feature_extractor.transform(test_data)\n","test_dep_data = text_feature_extractor.transform(test_dep_data)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3060,"status":"aborted","timestamp":1713253355970,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"BMIaEL70zBiM"},"outputs":[],"source":["def normalise_text(text):\n","    text = fix_text(text)\n","    text = text.lower()  # lowercase\n","    text = text.translate(str.maketrans('', '', string.punctuation))    # remove punctuation\n","    text = re.sub(r'\\s{2,}', ' ', text)   # replace more than or equal to two white spaces into one white space.\n","    return text"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3060,"status":"aborted","timestamp":1713253355970,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"90AnjLIJ3s3U"},"outputs":[],"source":["# fix text\n","text_corpus['text'] = text_corpus['text'].apply(lambda text: fix_text(text))\n","# normalise text\n","text_corpus['text'] = text_corpus['text'].apply(lambda text: normalise_text(text))"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3060,"status":"aborted","timestamp":1713253355970,"user":{"displayName":"Janice","userId":"16402043149144267821"},"user_tz":-480},"id":"4e5OnINd7eF1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>school homework clubs become increasingly popu...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>widely accepted knowledge great source power s...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>first impressions great power shape interactio...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>name address city state zip code email address...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>limiting car usage numerous advantages benefit...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  school homework clubs become increasingly popu...    1.0\n","1  widely accepted knowledge great source power s...    1.0\n","2  first impressions great power shape interactio...    1.0\n","3  name address city state zip code email address...    1.0\n","4  limiting car usage numerous advantages benefit...    1.0"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["text_corpus.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Word2Vec + Logistic Regression"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>%unique_word_total</th>\n","      <th>%stop_word_total</th>\n","      <th>mean_word_length</th>\n","      <th>mean_char_count_per_word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>school homework clubs become increasingly popu...</td>\n","      <td>1.0</td>\n","      <td>0.625000</td>\n","      <td>0.0</td>\n","      <td>6.812500</td>\n","      <td>7.802083</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>widely accepted knowledge great source power s...</td>\n","      <td>1.0</td>\n","      <td>0.642857</td>\n","      <td>0.0</td>\n","      <td>6.980519</td>\n","      <td>7.974026</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>first impressions great power shape interactio...</td>\n","      <td>1.0</td>\n","      <td>0.803738</td>\n","      <td>0.0</td>\n","      <td>7.168224</td>\n","      <td>8.158879</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>name address city state zip code email address...</td>\n","      <td>1.0</td>\n","      <td>0.592920</td>\n","      <td>0.0</td>\n","      <td>7.274336</td>\n","      <td>8.271386</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>limiting car usage numerous advantages benefit...</td>\n","      <td>1.0</td>\n","      <td>0.632867</td>\n","      <td>0.0</td>\n","      <td>7.202797</td>\n","      <td>8.199301</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label  \\\n","0  school homework clubs become increasingly popu...    1.0   \n","1  widely accepted knowledge great source power s...    1.0   \n","2  first impressions great power shape interactio...    1.0   \n","3  name address city state zip code email address...    1.0   \n","4  limiting car usage numerous advantages benefit...    1.0   \n","\n","   %unique_word_total  %stop_word_total  mean_word_length  \\\n","0            0.625000               0.0          6.812500   \n","1            0.642857               0.0          6.980519   \n","2            0.803738               0.0          7.168224   \n","3            0.592920               0.0          7.274336   \n","4            0.632867               0.0          7.202797   \n","\n","   mean_char_count_per_word  \n","0                  7.802083  \n","1                  7.974026  \n","2                  8.158879  \n","3                  8.271386  \n","4                  8.199301  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["from gensim.models import Word2Vec\n","from sklearn.model_selection import train_test_split\n","\n","# Splitting data back into training and test datasets\n","test_data = test_data  # already defined in the previous code\n","train_data.shape, test_data.shape\n","train_data.head()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Tokenize the text in each dataset\n","train_data['tokenized'] = train_data['text'].apply(word_tokenize)\n","test_data['tokenized'] = test_data['text'].apply(word_tokenize)\n","test_dep_data['tokenized'] = test_dep_data['text'].apply(word_tokenize)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Train a Word2Vec model\n","model_w2v = Word2Vec(sentences=train_data['tokenized'], vector_size=100, window=5, min_count=1, workers=6)\n","\n","# Convert text to a mean vector\n","def document_vector(word2vec_model, doc):\n","    # remove out-of-vocabulary words\n","    doc = [word for word in doc if word in word2vec_model.wv.index_to_key]\n","    if len(doc) == 0:\n","        return np.zeros(word2vec_model.vector_size)\n","    return np.mean(word2vec_model.wv[doc], axis=0)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>%unique_word_total</th>\n","      <th>%stop_word_total</th>\n","      <th>mean_word_length</th>\n","      <th>mean_char_count_per_word</th>\n","      <th>tokenized</th>\n","      <th>doc_vector</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>school homework clubs become increasingly popu...</td>\n","      <td>1.0</td>\n","      <td>0.625000</td>\n","      <td>0.0</td>\n","      <td>6.812500</td>\n","      <td>7.802083</td>\n","      <td>[school, homework, clubs, become, increasingly...</td>\n","      <td>[0.57162076, -0.06686906, 0.72704774, 0.841681...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>widely accepted knowledge great source power s...</td>\n","      <td>1.0</td>\n","      <td>0.642857</td>\n","      <td>0.0</td>\n","      <td>6.980519</td>\n","      <td>7.974026</td>\n","      <td>[widely, accepted, knowledge, great, source, p...</td>\n","      <td>[0.6422472, -0.5788288, 1.2735754, 0.17931908,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>first impressions great power shape interactio...</td>\n","      <td>1.0</td>\n","      <td>0.803738</td>\n","      <td>0.0</td>\n","      <td>7.168224</td>\n","      <td>8.158879</td>\n","      <td>[first, impressions, great, power, shape, inte...</td>\n","      <td>[0.5557356, -0.5109672, 1.1807613, -0.03543265...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>name address city state zip code email address...</td>\n","      <td>1.0</td>\n","      <td>0.592920</td>\n","      <td>0.0</td>\n","      <td>7.274336</td>\n","      <td>8.271386</td>\n","      <td>[name, address, city, state, zip, code, email,...</td>\n","      <td>[-0.67589957, 0.2583637, 0.5094211, -0.0832677...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>limiting car usage numerous advantages benefit...</td>\n","      <td>1.0</td>\n","      <td>0.632867</td>\n","      <td>0.0</td>\n","      <td>7.202797</td>\n","      <td>8.199301</td>\n","      <td>[limiting, car, usage, numerous, advantages, b...</td>\n","      <td>[0.59118575, 0.024846703, 0.9679171, 1.8717972...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label  \\\n","0  school homework clubs become increasingly popu...    1.0   \n","1  widely accepted knowledge great source power s...    1.0   \n","2  first impressions great power shape interactio...    1.0   \n","3  name address city state zip code email address...    1.0   \n","4  limiting car usage numerous advantages benefit...    1.0   \n","\n","   %unique_word_total  %stop_word_total  mean_word_length  \\\n","0            0.625000               0.0          6.812500   \n","1            0.642857               0.0          6.980519   \n","2            0.803738               0.0          7.168224   \n","3            0.592920               0.0          7.274336   \n","4            0.632867               0.0          7.202797   \n","\n","   mean_char_count_per_word  \\\n","0                  7.802083   \n","1                  7.974026   \n","2                  8.158879   \n","3                  8.271386   \n","4                  8.199301   \n","\n","                                           tokenized  \\\n","0  [school, homework, clubs, become, increasingly...   \n","1  [widely, accepted, knowledge, great, source, p...   \n","2  [first, impressions, great, power, shape, inte...   \n","3  [name, address, city, state, zip, code, email,...   \n","4  [limiting, car, usage, numerous, advantages, b...   \n","\n","                                          doc_vector  \n","0  [0.57162076, -0.06686906, 0.72704774, 0.841681...  \n","1  [0.6422472, -0.5788288, 1.2735754, 0.17931908,...  \n","2  [0.5557356, -0.5109672, 1.1807613, -0.03543265...  \n","3  [-0.67589957, 0.2583637, 0.5094211, -0.0832677...  \n","4  [0.59118575, 0.024846703, 0.9679171, 1.8717972...  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["train_data['doc_vector'] = train_data['tokenized'].apply(lambda x: document_vector(model_w2v, x))\n","test_data['doc_vector'] = test_data['tokenized'].apply(lambda x: document_vector(model_w2v, x))\n","test_dep_data['doc_vector'] = test_dep_data['tokenized'].apply(lambda x: document_vector(model_w2v, x))\n","train_data.head()\n","\n","# save the train and test data as pickle file\n","# train_data.to_pickle('train_data.pkl')\n","# test_data.to_pickle('test_data.pkl')"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.metrics import roc_auc_score, mean_squared_error\n","from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n","\n","# load pickles to save time from running all above code\n","# train_data = pd.read_pickle('train_data.pkl')\n","# test_data = pd.read_pickle('test_data.pkl')\n","\n","# Extract feature vectors for training and testing\n","X_train = np.array(list(train_data['doc_vector']))\n","X_test = np.array(list(test_data['doc_vector']))\n","X_test_dep = np.array(list(test_dep_data['doc_vector']))\n","y_train = train_data['label'].values\n","y_test = test_data['label'].values\n","y_test_dep = test_dep_data['label'].values"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression - Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.45      0.55        20\n","           1       0.59      0.80      0.68        20\n","\n","    accuracy                           0.62        40\n","   macro avg       0.64      0.62      0.61        40\n","weighted avg       0.64      0.62      0.61        40\n","\n","Logistic Regression - Confusion Matrix:\n","[[ 9 11]\n"," [ 4 16]]\n","AUC: 0.625\n","Precision: 0.5925925925925926\n","Recall: 0.8\n"]}],"source":["# Train a logistic regression model\n","log_reg = LogisticRegression(max_iter=50000) # set max_iter=50000 as the model does not converge with default value\n","log_reg.fit(X_train, y_train)\n","import pickle\n","\n","# save the model to disk\n","filename = 'logisticregression_model_trainv2.sav'\n","pickle.dump(log_reg, open(filename, 'wb'))\n","\n","# Predictions and evaluations\n","y_pred = log_reg.predict(X_test)\n","print(\"Logistic Regression - Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"Logistic Regression - Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))\n","print('AUC:', roc_auc_score(y_test, y_pred))\n","# print precision, recall, and precision-recall curve\n","print('Precision:', precision_score(y_test, y_pred))\n","print('Recall:', recall_score(y_test, y_pred))\n","precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n","\n","# print(\"\\n=========== test_dep =================\\n\")\n","\n","# y_pred_dep = log_reg.predict(X_test_dep)\n","# print(\"Logistic Regression - Classification Report:\")\n","# print(classification_report(y_test_dep, y_pred_dep))\n","# print(\"Logistic Regression - Confusion Matrix:\")\n","# print(confusion_matrix(y_test_dep, y_pred_dep))\n","# print('AUC:', roc_auc_score(y_test_dep, y_pred_dep))\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# # Train a LightGBM model\n","# # choose LightGBM over SVC & RandomForest as it is more scalable and faster to train\n","# import lightgbm as lgb\n","# lgbm = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.05, max_depth=-1)\n","# lgbm.fit(X_train, y_train)\n","\n","# # Predictions and evaluations\n","# y_pred = lgbm.predict(X_test)\n","# print(\"LightGBM - Classification Report:\")\n","# print(classification_report(y_test, y_pred))\n","# print(\"LightGBM - Confusion Matrix:\")\n","# print(confusion_matrix(y_test, y_pred))\n","# print('AUC:', roc_auc_score(y_test, y_pred))\n","\n","# print(\"\\n=========== test_dep =================\\n\")\n","\n","# y_pred_dep = lgbm.predict(X_test_dep)\n","# print(\"LightGBM - Classification Report:\")\n","# print(classification_report(y_test_dep, y_pred_dep))\n","# print(\"LightGBM - Confusion Matrix:\")\n","# print(confusion_matrix(y_test_dep, y_pred_dep))\n","# print('AUC:', roc_auc_score(y_test_dep, y_pred_dep))\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# # Train a CatBoost model\n","# from catboost import CatBoostClassifier\n","# catboost = CatBoostClassifier(iterations=100, learning_rate=0.05, depth=5)\n","# catboost.fit(X_train, y_train)\n","\n","# # Predictions and evaluations\n","# y_pred = catboost.predict(X_test)\n","# print(\"CatBoost - Classification Report:\")\n","# print(classification_report(y_test, y_pred))\n","# print(\"CatBoost - Confusion Matrix:\")\n","# print(confusion_matrix(y_test, y_pred))\n","# print('AUC:', roc_auc_score(y_test, y_pred))\n","\n","# print(\"\\n=========== test_dep =================\\n\")\n","\n","# y_pred_dep = catboost.predict(X_test_dep)\n","# print(\"CatBoost - Classification Report:\")\n","# print(classification_report(y_test_dep, y_pred_dep))\n","# print(\"CatBoost - Confusion Matrix:\")\n","# print(confusion_matrix(y_test_dep, y_pred_dep))\n","# print('AUC:', roc_auc_score(y_test_dep, y_pred_dep))\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
