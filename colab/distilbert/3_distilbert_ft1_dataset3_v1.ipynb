{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ZDJ_iLd7Rw8b"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"6d4a7jGOGjgH"},"source":["# DistilBERT"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POHqQm2tGlaD","outputId":"dcdd4899-f23f-4e28-eeac-6c37a8bcffa2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["! pip install --quiet lightning\n","! pip install --quiet transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jLw_iPGcGmom"},"outputs":[],"source":["import pandas as pd\n","# importing all necessary packages\n","from os import listdir\n","from os.path import join\n","from sklearn.model_selection import train_test_split\n","import string\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torchmetrics\n","import pytorch_lightning as pl\n","from transformers import AutoModelForSequenceClassification\n","from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from transformers import DistilBertTokenizer"]},{"cell_type":"markdown","metadata":{},"source":["#### Dataset Preparation"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":7800,"status":"ok","timestamp":1713347170533,"user":{"displayName":"YX (maomao)","userId":"05973236653074657729"},"user_tz":-480},"id":"yuXxIHJVIMoZ","outputId":"e5a77dda-dd73-4dd3-da7b-54475091cc9f"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>%unique_word_total</th>\n","      <th>%stop_word_total</th>\n","      <th>mean_word_length</th>\n","      <th>mean_char_count_per_word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>carfree cities become subject increasing inter...</td>\n","      <td>1</td>\n","      <td>0.466087</td>\n","      <td>0.340870</td>\n","      <td>5.926957</td>\n","      <td>6.925217</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>car free cities carfree cities concept gaining...</td>\n","      <td>1</td>\n","      <td>0.518519</td>\n","      <td>0.309942</td>\n","      <td>6.111111</td>\n","      <td>7.109162</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sustainable urban future carfree cities emergi...</td>\n","      <td>1</td>\n","      <td>0.492188</td>\n","      <td>0.302734</td>\n","      <td>6.265625</td>\n","      <td>7.265625</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pioneering sustainable urban living era marked...</td>\n","      <td>1</td>\n","      <td>0.495183</td>\n","      <td>0.333333</td>\n","      <td>6.015414</td>\n","      <td>7.015414</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>path sustainable urban living age rapid urbani...</td>\n","      <td>1</td>\n","      <td>0.481409</td>\n","      <td>0.315068</td>\n","      <td>6.039139</td>\n","      <td>7.039139</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label  \\\n","0  carfree cities become subject increasing inter...      1   \n","1  car free cities carfree cities concept gaining...      1   \n","2  sustainable urban future carfree cities emergi...      1   \n","3  pioneering sustainable urban living era marked...      1   \n","4  path sustainable urban living age rapid urbani...      1   \n","\n","   %unique_word_total  %stop_word_total  mean_word_length  \\\n","0            0.466087          0.340870          5.926957   \n","1            0.518519          0.309942          6.111111   \n","2            0.492188          0.302734          6.265625   \n","3            0.495183          0.333333          6.015414   \n","4            0.481409          0.315068          6.039139   \n","\n","   mean_char_count_per_word  \n","0                  6.925217  \n","1                  7.109162  \n","2                  7.265625  \n","3                  7.015414  \n","4                  7.039139  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#show dataframe\n","train_data_preprocessed = pd.read_csv('./Data/kaggle_preprocessed.csv')\n","train_data_preprocessed.head()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["(27340, 6)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_data_preprocessed.shape"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>%unique_word_total</th>\n","      <th>%stop_word_total</th>\n","      <th>mean_word_length</th>\n","      <th>mean_char_count_per_word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>marshall plan test progress test progress whet...</td>\n","      <td>0</td>\n","      <td>0.444112</td>\n","      <td>0.392216</td>\n","      <td>5.384232</td>\n","      <td>6.384232</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>promoting global regional security postcold wa...</td>\n","      <td>0</td>\n","      <td>0.414807</td>\n","      <td>0.424949</td>\n","      <td>5.150101</td>\n","      <td>6.150101</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>womanhood peacemaking taking advantage unity c...</td>\n","      <td>0</td>\n","      <td>0.475947</td>\n","      <td>0.378710</td>\n","      <td>5.583419</td>\n","      <td>6.583419</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>human rights universal western construct cruci...</td>\n","      <td>0</td>\n","      <td>0.370482</td>\n","      <td>0.503012</td>\n","      <td>4.875502</td>\n","      <td>5.875502</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>challenges american foreign service rebuilding...</td>\n","      <td>0</td>\n","      <td>0.467425</td>\n","      <td>0.379524</td>\n","      <td>5.668046</td>\n","      <td>6.668046</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label  \\\n","0  marshall plan test progress test progress whet...      0   \n","1  promoting global regional security postcold wa...      0   \n","2  womanhood peacemaking taking advantage unity c...      0   \n","3  human rights universal western construct cruci...      0   \n","4  challenges american foreign service rebuilding...      0   \n","\n","   %unique_word_total  %stop_word_total  mean_word_length  \\\n","0            0.444112          0.392216          5.384232   \n","1            0.414807          0.424949          5.150101   \n","2            0.475947          0.378710          5.583419   \n","3            0.370482          0.503012          4.875502   \n","4            0.467425          0.379524          5.668046   \n","\n","   mean_char_count_per_word  \n","0                  6.384232  \n","1                  6.150101  \n","2                  6.583419  \n","3                  5.875502  \n","4                  6.668046  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#show dataframe\n","test_data_preprocessed = pd.read_csv('./Data/new_essay_val_preprocessed.csv')\n","test_data_preprocessed.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"o-0etm5MGmwA"},"outputs":[{"name":"stdout","output_type":"stream","text":["(19138, 6)\n","(8202, 6)\n","(40, 6)\n","(19137, 6)\n","(8202, 6)\n","(40, 6)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","#show shape of train and test set\n","df_train, df_validation = train_test_split(train_data_preprocessed, test_size=0.3, random_state=42)\n","df_test = test_data_preprocessed # rename\n","\n","print(df_train.shape)\n","print(df_validation.shape)\n","print(df_test.shape)\n","df_train.dropna(inplace = True)\n","df_validation.dropna(inplace = True)\n","df_test.dropna(inplace = True)\n","print(df_train.shape)\n","print(df_validation.shape)\n","print(df_test.shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-PeuA5DwllIP"},"outputs":[],"source":["# Prepare for DataLoader and Dataset without converting labels to ordinal\n","X_train_token_list = []\n","X_val_token_list = []\n","X_test_token_list = []\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Process the data\n","for i in df_train[\"text\"]:\n","    X_train_token_list.append(tokenizer(i, truncation=True, padding='max_length', max_length=512, return_tensors='pt'))\n","for i in df_validation[\"text\"]:\n","    X_val_token_list.append(tokenizer(i, truncation=True, padding='max_length', max_length=512, return_tensors='pt'))\n","for i in df_test[\"text\"]:\n","    X_test_token_list.append(tokenizer(i, truncation=True, padding='max_length', max_length=512, return_tensors='pt'))\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"QI3jx65tG7zJ"},"outputs":[],"source":["# add labels to token_list for use later as dataset \n","train_labels = []\n","for i in df_train[\"label\"]:\n","  if int(i) == 0:\n","    train_labels.append(0)\n","  else:\n","    train_labels.append(1)\n","val_labels = []\n","for i in df_validation[\"label\"]:\n","  if int(i) == 0:\n","    val_labels.append(0)\n","  else:\n","    val_labels.append(1)\n","test_labels = []\n","for i in df_test[\"label\"]:\n","  if int(i) == 0:\n","    test_labels.append(0)\n","  else:\n","    test_labels.append(1)\n","for i in range(len(X_train_token_list)):\n","  X_train_token_list[i][\"label\"] = torch.tensor(train_labels[i])\n","for i in range(len(X_val_token_list)):\n","  X_val_token_list[i][\"label\"] = torch.tensor(val_labels[i])\n","for i in range(len(X_test_token_list)):\n","  X_test_token_list[i][\"label\"] = torch.tensor(test_labels[i])\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"6-NThQwOHE8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[  101,  6203,  3836, 18442,  3047,  1996,  2869,  2724,  4550,  2451,\n","          2326,  2157,  2157,  2518,  2451,  2326,  2590,  2451,  2925,  2451,\n","          2689,  7955,  3407,  2111,  2987,  2102, 10587,  4783,  2393,  2477,\n","          2518,  2082,  2493,  2451,  2326,  2034,  3114,  2451,  2326,  2590,\n","          3664,  2103,  7098,  2111,  2360,  6752,  2451,  2326,  2272,  2393,\n","          2187, 11669,  2101,  5437,  2919,  2111,  6283,  3047,  3531,  2123,\n","          2102,  2681, 27042,  2723,  2404, 11669,  2117,  3114,  2045,  2015,\n","          2724,  2111,  2147,  4550,  2724,  4627,  2111,  2228,  3138,  2172,\n","          2051,  2185,  2215,  2393,  2111,  2451,  2082,  2518,  2814,  2987,\n","          2102,  2215,  2393,  2197,  3114,  2045,  2015,  2172,  4550,  2305,\n","          5553, 27287,  2342,  4550,  2305,  2279,  2154,  3310,  4550, 15708,\n","          2111,  4550,  4933,  2894,  2823,  2362,  2228,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]]), 'label': tensor(0)}\n"]}],"source":["# show our input data\n","print(X_train_token_list[1])"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"15k3tqRgHNPJ"},"outputs":[],"source":["# Create dataset and dataloader\n","class MyDataset(Dataset):\n","    def __init__(self, array_of_dicts):\n","        self.data = array_of_dicts\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        return item\n","\n","trainset = MyDataset(X_train_token_list)\n","valset = MyDataset(X_val_token_list)\n","testset = MyDataset(X_test_token_list)\n","\n","train_loader = DataLoader(\n","    dataset=trainset,\n","    batch_size=4,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = DataLoader(\n","    dataset=valset,\n","    batch_size=4,\n","    num_workers=0\n",")\n","\n","test_loader = DataLoader(\n","    dataset=testset,\n","    batch_size=4,\n","    num_workers=0\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Uh3qDzdWHP7l"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# train our model and freeze our parameter\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    \"distilbert-base-uncased\", num_labels=2)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","for param in model.pre_classifier.parameters():\n","    param.requires_grad = True\n","\n","for param in model.classifier.parameters():\n","    param.requires_grad = True\n","\n","#model structure\n","class LightningModel(pl.LightningModule):\n","    def __init__(self, model, learning_rate=5e-5):\n","        super().__init__()\n","\n","        self.learning_rate = learning_rate\n","        self.model = model\n","\n","        # Ensure the config is set\n","        self.config = model.config  # Copy the model's configuration\n","\n","        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n","        self.train_auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=2)  # AUROC for training\n","        \n","\n","        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n","        self.val_auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=2)  # AUROC for evaluation\n","        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n","        self.test_auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=2)  # AUROC for testing\n","\n","    def forward(self, input_ids, attention_mask, labels):\n","        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n","\n","    def training_step(self, batch, batch_idx):\n","        outputs = self(batch[\"input_ids\"].squeeze(1), attention_mask=batch[\"attention_mask\"],\n","                       labels=batch[\"label\"])\n","        self.log(\"train_loss\", outputs[\"loss\"])\n","        logits = outputs[\"logits\"]\n","        probabilities = torch.softmax(logits, dim=1)  # Get probabilities for the positive class\n","        predicted_labels = torch.argmax(logits, 1)\n","\n","        # Log accuracy\n","        self.train_acc(predicted_labels, batch[\"label\"])\n","        self.log(\"train_acc\", self.train_acc)\n","    \n","        # Update and log AUROC for training\n","        self.train_auroc(probabilities, batch[\"label\"])\n","        # self.train_auroc.update(probabilities[:, 1], batch[\"label\"])  # Use probabilities for the positive class\n","        self.log(\"train_auroc\", self.train_auroc)\n","\n","        return outputs[\"loss\"]\n","\n","    def validation_step(self, batch, batch_idx):\n","        outputs = self(batch[\"input_ids\"].squeeze(1), attention_mask=batch[\"attention_mask\"],\n","                       labels=batch[\"label\"])\n","        self.log(\"val_loss\", outputs[\"loss\"], prog_bar=True)\n","\n","        logits = outputs[\"logits\"]\n","        probabilities = torch.softmax(logits, dim=1)  # Get probabilities\n","        predicted_labels = torch.argmax(logits, 1)\n","        \n","        # Log accuracy\n","        self.val_acc(predicted_labels, batch[\"label\"])\n","        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n","        \n","        # Update and log AUROC\n","        self.val_auroc(probabilities, batch[\"label\"])\n","        # self.val_auroc.update(probabilities[:, 1], batch[\"label\"])  # Use probabilities for the positive class\n","        self.log(\"val_auroc\", self.val_auroc, prog_bar=True)\n","        \n","        # output for classification report\n","        return {\"logits\": logits, \"loss\": outputs[\"loss\"], \"labels\": batch[\"label\"]}\n","\n","    def test_step(self, batch, batch_idx):\n","        outputs = self(batch[\"input_ids\"].squeeze(1), attention_mask=batch[\"attention_mask\"],\n","                       labels=batch[\"label\"])\n","        self.log(\"test_loss\", outputs[\"loss\"], prog_bar=True)\n","\n","        logits = outputs[\"logits\"]\n","        probabilities = torch.softmax(logits, dim=1)  # Get probabilities\n","        predicted_labels = torch.argmax(logits, 1)\n","\n","        # Log accuracy\n","        self.test_acc(predicted_labels, batch[\"label\"])\n","        self.log(\"accuracy\", self.test_acc, prog_bar=True)\n","\n","        # Update and log AUROC\n","        self.test_auroc(probabilities, batch[\"label\"])\n","        self.log(\"test_auroc\", self.test_auroc, prog_bar=True)\n","        \n","        # output for classification report\n","        return {\"logits\": logits, \"loss\": outputs[\"loss\"], \"labels\": batch[\"label\"]}\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","        return optimizer\n","\n","\n","lightning_model = LightningModel(model)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Change the monitor to \"val_auroc\"\n","logger = TensorBoardLogger(\"distilbert-add-fea/\", name=\"finetuning\", version=\"original\")\n","callbacks = [\n","    ModelCheckpoint(\n","        save_top_k=1, mode=\"max\", monitor=\"val_auroc\"  # Monitor the AUROC metric\n","    )\n","]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"MJoJH9aYHUOk"},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (mps), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","\n","  | Name        | Type                                | Params\n","--------------------------------------------------------------------\n","0 | model       | DistilBertForSequenceClassification | 67.0 M\n","1 | train_acc   | MulticlassAccuracy                  | 0     \n","2 | train_auroc | MulticlassAUROC                     | 0     \n","3 | val_acc     | MulticlassAccuracy                  | 0     \n","4 | val_auroc   | MulticlassAUROC                     | 0     \n","5 | test_acc    | MulticlassAccuracy                  | 0     \n","6 | test_auroc  | MulticlassAUROC                     | 0     \n","--------------------------------------------------------------------\n","592 K     Trainable params\n","66.4 M    Non-trainable params\n","67.0 M    Total params\n","267.820   Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5611a8393eb4eefa193fe83ceb37533","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |                                                                                            …"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64cad8d7f27e44a19476040065a55caa","version_major":2,"version_minor":0},"text/plain":["Training: |                                                                                                   …"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4836c42908b54db4b770f5f3e5cffa7b","version_major":2,"version_minor":0},"text/plain":["Validation: |                                                                                                 …"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3de5c4dece294a8289be2215c7292245","version_major":2,"version_minor":0},"text/plain":["Validation: |                                                                                                 …"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dab90a66ed044c81bfa5243f25b53d8d","version_major":2,"version_minor":0},"text/plain":["Validation: |                                                                                                 …"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=3` reached.\n"]}],"source":["trainer = pl.Trainer(\n","    max_epochs=3,\n","    callbacks=callbacks, \n","    # accelerator=\"gpu\",  uncomment if gpu available\n","    devices=1,\n","    logger=logger,\n","    log_every_n_steps=1,\n",")\n","\n","# Start training\n","trainer.fit(\n","    model=lightning_model,\n","    train_dataloaders=train_loader,\n","    val_dataloaders=val_loader\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Define the path to save the model\n","model_path = \"./distilbert-dataset-v3-kaggle30k_ep3_dropout0.4.pth\"\n","\n","# Save the entire model\n","torch.save(lightning_model, model_path)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"YjtqnqNmHWxZ"},"outputs":[{"name":"stderr","output_type":"stream","text":["Restoring states from the checkpoint path at distilbert-add-fea/finetuning/original/checkpoints/epoch=2-step=14355-v2.ckpt\n","Loaded model weights from the checkpoint at distilbert-add-fea/finetuning/original/checkpoints/epoch=2-step=14355-v2.ckpt\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41da4ef7d62842f1a84fd43b0e0ad5f2","version_major":2,"version_minor":0},"text/plain":["Validation: |                                                                                                 …"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","     Validate metric           DataLoader 0\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","         val_acc             0.980248749256134\n","        val_auroc           0.9968925714492798\n","        val_loss            0.05967448651790619\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"]},{"data":{"text/plain":["[{'val_loss': 0.05967448651790619,\n","  'val_acc': 0.980248749256134,\n","  'val_auroc': 0.9968925714492798}]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# test with validation dataset\n","val_results = trainer.validate(lightning_model, dataloaders=val_loader, ckpt_path=\"best\")\n","val_results"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"931ZPHv1HXT0"},"outputs":[{"name":"stderr","output_type":"stream","text":["Restoring states from the checkpoint path at distilbert-add-fea/finetuning/original/checkpoints/epoch=2-step=14355-v2.ckpt\n","Loaded model weights from the checkpoint at distilbert-add-fea/finetuning/original/checkpoints/epoch=2-step=14355-v2.ckpt\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"026c699f762242c78e2e3917bbe5d236","version_major":2,"version_minor":0},"text/plain":["Testing: |                                                                                                    …"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","       Test metric             DataLoader 0\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","        accuracy             0.675000011920929\n","       test_auroc           0.8899999856948853\n","        test_loss            1.382774829864502\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"]},{"data":{"text/plain":["[{'test_loss': 1.382774829864502,\n","  'accuracy': 0.675000011920929,\n","  'test_auroc': 0.8899999856948853}]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["test_results = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")\n","test_results"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 658), started 7:14:09 ago. (Use '!kill 658' to kill it.)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-2e9895c3fc57dc42\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-2e9895c3fc57dc42\");\n","          const url = new URL(\"http://localhost\");\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Start tensorboard.\n","%load_ext tensorboard\n","%tensorboard --logdir distilbert/finetuning/original/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}
